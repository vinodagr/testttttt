# hypothesis-1 and 2
from sklearn import model_selection
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score,confusion_matrix
import seaborn as sns
import time
# Load the data
data = pd.read_csv('C:\\Users\\Vinod A\\Desktop\\test.csv',encoding='latin-1')
# Apply label encoding to categorical variables,
label_encoder = LabelEncoder()
x = data.drop(columns=['Campus Placement'])
y = data['Campus Placement']
x_encoded = x.copy()
for col in x_encoded.columns:
    if x_encoded[col].dtype == 'object':
        x_encoded[col] = label_encoder.fit_transform(x_encoded[col])

# Apply one-hot encoding to categorical variables
onehot_encoder = OneHotEncoder()
x_encoded = pd.get_dummies(x_encoded)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x_encoded, y, test_size=0.2, random_state=42)


# Initialize the models
dt = DecisionTreeClassifier()
rf = RandomForestClassifier()
lr = LogisticRegression()
nb = GaussianNB()
knn = KNeighborsClassifier()
svc = SVC()
xgb = XGBClassifier()
ada = AdaBoostClassifier(base_estimator=dt)

# Train the models
dt.fit(x_train, y_train)
rf.fit(x_train, y_train)
lr.fit(x_train, y_train)
nb.fit(x_train, y_train)
knn.fit(x_train, y_train)
svc.fit(x_train, y_train)
xgb.fit(x_train, y_train)
ada.fit(x_train, y_train)




# Evaluate the models
print(f'Decision Tree Accuracy: {dt.score(x_test, y_test)}')
print(f'Random Forest Accuracy: {rf.score(x_test, y_test)}')
print(f'Logistic Regression Accuracy: {lr.score(x_test, y_test)}')
print(f'Naive Bayes Accuracy: {nb.score(x_test, y_test)}')
print(f'KNN Accuracy: {knn.score(x_test, y_test)}')
print(f'SVM Accuracy: {svc.score(x_test, y_test)}')
print(f'XGBoost Accuracy: {xgb.score(x_test, y_test)}')
print(f'AdaBoost Accuracy: {ada.score(x_test, y_test)}')

models = [dt, rf, lr, nb, knn, svc, xgb, ada]
model_names = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'Naive Bayes', 'KNN', 'SVM', 'XGBoost', 'AdaBoost']
for i in range(len(models)):
    # Train the model
    start_time = time.time()
    models[i].fit(x_train, y_train)
    end_time = time.time()
    build_time = end_time - start_time
   
    # Make predictions on the test set
    y_pred = models[i].predict(x_test)

    # Calculate the accuracy score
    accuracy = accuracy_score(y_test, y_pred)
    print(f'{model_names[i]} Accuracy: {accuracy:.2f}')
    print("Build Time:", build_time, "seconds")
    # Calculate the confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot the heatmap
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, cmap='Blues')
    plt.title(f'{model_names[i]} Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()
